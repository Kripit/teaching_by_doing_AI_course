# Chapter 04: Regularization & Generalization
## Making Your Neural Networks Actually Work on Real Data

<div align="center">

**The Art of Building Models That Generalize**

*From Overfitting Monster to Production-Ready Model*

</div>

---

## üéØ Chapter Objectives

By the end of this chapter, you will:

- ‚úÖ Understand **overfitting vs underfitting** at a deep level
- ‚úÖ Master **L1 and L2 regularization** (weight decay) with mathematical intuition
- ‚úÖ Implement **Dropout** from scratch and understand why it works
- ‚úÖ Master **Batch Normalization** - the technique that revolutionized deep learning
- ‚úÖ Implement **Early Stopping** and other practical techniques
- ‚úÖ Understand **data augmentation** strategies
- ‚úÖ Build an **overfit detector** that diagnoses model problems
- ‚úÖ Achieve **better generalization** on test data

**Real-world impact**: These techniques are the difference between:
- üìâ Model that memorizes training data (95% train, 60% test)
- üìà Model that generalizes (92% train, 88% test) ‚úì **Production-ready!**

---

## üìñ Table of Contents

1. [The Generalization Problem](#1-the-generalization-problem)
2. [Overfitting vs Underfitting](#2-overfitting-vs-underfitting)
3. [L1 and L2 Regularization](#3-l1-and-l2-regularization)
4. [Dropout](#4-dropout)
5. [Batch Normalization](#5-batch-normalization)
6. [Early Stopping](#6-early-stopping)
7. [Data Augmentation](#7-data-augmentation)
8. [Other Regularization Techniques](#8-other-regularization-techniques)
9. [Practical Guidelines](#9-practical-guidelines)
10. [Implementation Strategy](#10-implementation-strategy)

---

## 1. The Generalization Problem

### 1.1 What is Generalization?

**Generalization** = Model's ability to perform well on **unseen data**

**The fundamental goal of machine learning:**
```
Train on: 60,000 MNIST training images
Test on: 10,000 MNIST test images (never seen during training!)

Good model:
- Training accuracy: 99.2%
- Test accuracy: 98.9%  ‚úì Generalizes well!

Bad model:
- Training accuracy: 99.9%
- Test accuracy: 85.3%  ‚úó Overfitting!
```

### 1.2 The Bias-Variance Tradeoff

**Two sources of error:**

1. **Bias** = Error from wrong assumptions
   - Model is too simple to capture the true pattern
   - **Underfitting**: Model can't even fit training data well
   - Example: Linear model for non-linear data

2. **Variance** = Error from sensitivity to training data
   - Model is too complex and learns noise
   - **Overfitting**: Fits training data perfectly but fails on new data
   - Example: 100-layer network for simple task

**The tradeoff:**
```
Total Error = Bias¬≤ + Variance + Irreducible Noise

Simple model ‚Üí High bias, Low variance (underfits)
Complex model ‚Üí Low bias, High variance (overfits)
Sweet spot ‚Üí Balanced bias and variance (generalizes!)
```

### 1.3 Why Neural Networks Overfit

**Neural networks are EXTREMELY powerful:**

- Can approximate **any function** (Universal Approximation Theorem)
- Have **millions of parameters** (more than training samples!)
- Can **memorize** the entire training set

**Example of memorization:**

Say you have 50,000 training images:
- Network with 10 million parameters
- Can assign unique "ID" to each image and memorize labels
- 100% training accuracy, but 10% test accuracy (random guessing!)

**This is why regularization is CRITICAL.**

---

## 2. Overfitting vs Underfitting

### 2.1 Visual Recognition

**Training vs Validation Loss Curves:**

```
Loss
  ‚îÇ
  ‚îÇ  Underfitting (High bias)
  ‚îÇ  ‚ï±‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ Train loss (high)
  ‚îÇ ‚ï±
  ‚îÇ‚ï± ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ Val loss (high)
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Epochs
  
  Model: Too simple (1 hidden layer, 10 neurons)
  Both train and val loss are high
  Solution: Make model more complex
  
Loss
  ‚îÇ
  ‚îÇ  Good fit (Just right!)
  ‚îÇ  ‚ï≤
  ‚îÇ   ‚ï≤_______________  Train loss (low)
  ‚îÇ    ‚ï≤______________ Val loss (low, similar to train)
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Epochs
  
  Model: Appropriate complexity
  Train and val loss are both low and close
  Solution: This is what we want!
  
Loss
  ‚îÇ
  ‚îÇ  Overfitting (High variance)
  ‚îÇ  ‚ï≤
  ‚îÇ   ‚ï≤_______________  Train loss (very low)
  ‚îÇ    ‚ï≤
  ‚îÇ     ‚Äæ‚Äæ‚ï≤___________  Val loss (starts low, then increases!)
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Epochs
  
  Model: Too complex (20 layers, no regularization)
  Train loss keeps decreasing, but val loss increases
  Solution: Add regularization!
```

### 2.2 Detecting Overfitting

**Key signals:**

1. **Training vs Validation Gap**
   - Train accuracy: 99%
   - Val accuracy: 85%
   - Gap = 14% ‚Üí **Overfitting!**

2. **Validation Loss Increases**
   - Epoch 10: Val loss = 0.5
   - Epoch 20: Val loss = 0.4
   - Epoch 30: Val loss = 0.6 ‚Üë **Overfitting started!**

3. **Model Complexity Indicators**
   - Too many layers for simple task
   - Very large weights (|w| > 100)
   - High training accuracy too quickly (epoch 2: 98%)

### 2.3 Detecting Underfitting

**Key signals:**

1. **Low Performance on Both**
   - Train accuracy: 65%
   - Val accuracy: 63%
   - Both are bad ‚Üí **Underfitting!**

2. **Loss Plateaus Early**
   - Epoch 5: Loss = 1.2
   - Epoch 50: Loss = 1.1
   - Barely improving ‚Üí Model can't learn

3. **Model Too Simple**
   - 1-layer network for complex image classification
   - Linear model for non-linear data

---

## 3. L1 and L2 Regularization

### 3.1 The Core Idea

**Add a penalty term to the loss function** that discourages large weights:

$$L_{total} = L_{data} + \lambda \cdot R(w)$$

**Breaking down EVERY symbol:**

- $L_{total}$ = **Total loss** to minimize (what we actually optimize)
  - Combines data loss and regularization penalty
  
- $=$ = **Assignment**

- $L_{data}$ = **Data loss** (original loss)
  - Cross-entropy, MSE, etc.
  - Measures how well model fits training data
  - Example: If predictions are perfect ‚Üí $L_{data} = 0$
  
- $+$ = **Addition**

- $\lambda$ = **Regularization strength** (Greek letter "lambda")
  - Hyperparameter controlling tradeoff
  - Typical values: 0.0001, 0.001, 0.01, 0.1
  - $\lambda = 0$ ‚Üí no regularization
  - $\lambda$ large ‚Üí strong regularization (simpler model)
  - Pronounced "lambda"
  
- $\cdot$ = **Multiplication**

- $R(w)$ = **Regularization term** (penalty function)
  - Measures "complexity" of weights
  - Different forms: L1, L2, elastic net
  - $w$ = all weights in the network

**Why this works:**

The optimizer now faces two competing objectives:
1. **Minimize $L_{data}$** ‚Üí Fit the training data well
2. **Minimize $R(w)$** ‚Üí Keep weights small

The parameter $\lambda$ controls the balance:
- If $\lambda$ is too small ‚Üí Model overfits (ignores regularization)
- If $\lambda$ is too large ‚Üí Model underfits (weights stay too small)

### 3.2 L2 Regularization (Weight Decay)

**Most popular regularization technique!**

$$R(w) = \frac{1}{2} \sum_{i} w_i^2 = \frac{1}{2} ||w||_2^2$$

**Breaking down EVERY symbol:**

- $R(w)$ = **L2 regularization term**
  - "L2" because it uses the L2 norm (Euclidean norm)
  - Also called "weight decay" or "ridge regression"
  
- $=$ = **Equals**

- $\frac{1}{2}$ = **Constant factor of 0.5**
  - Makes derivative cleaner: $\frac{d}{dw}(\frac{1}{2}w^2) = w$
  - Optional (can be absorbed into $\lambda$)
  
- $\sum_{i}$ = **Sum over all weights**
  - $i$ = index going through every weight in network
  - Includes weights from ALL layers
  - Does NOT include biases (typically)
  
- $w_i^2$ = **Squared weight value**
  - If $w_i = 3.0$ ‚Üí $w_i^2 = 9.0$
  - If $w_i = -3.0$ ‚Üí $w_i^2 = 9.0$ (same penalty!)
  - Always positive (can't cancel out)
  - Large weights penalized heavily (quadratic growth)
  
- $||w||_2^2$ = **Squared L2 norm** (alternative notation)
  - $||w||_2 = \sqrt{w_1^2 + w_2^2 + ... + w_n^2}$ (Euclidean length)
  - $||w||_2^2 = w_1^2 + w_2^2 + ... + w_n^2$ (squared length)
  - Same as $\sum_i w_i^2$

**Concrete numerical example:**

Say we have 4 weights: $w = [2.0, -1.5, 0.5, -3.0]$

L2 regularization term:

$$R(w) = \frac{1}{2}(2.0^2 + (-1.5)^2 + 0.5^2 + (-3.0)^2)$$

$$R(w) = \frac{1}{2}(4.0 + 2.25 + 0.25 + 9.0)$$

$$R(w) = \frac{1}{2}(15.5) = 7.75$$

If $\lambda = 0.01$:

$$\lambda \cdot R(w) = 0.01 \times 7.75 = 0.0775$$

This gets added to the data loss!

**Gradient of L2 regularization:**

$$\frac{\partial}{\partial w_i} \left(\frac{\lambda}{2} w_i^2\right) = \lambda w_i$$

**What this means for weight updates:**

Without regularization:

$$w_i := w_i - \eta \frac{\partial L_{data}}{\partial w_i}$$

With L2 regularization:

$$w_i := w_i - \eta \left(\frac{\partial L_{data}}{\partial w_i} + \lambda w_i\right)$$

Rearranging:

$$w_i := w_i(1 - \eta\lambda) - \eta \frac{\partial L_{data}}{\partial w_i}$$

The term $(1 - \eta\lambda)$ **shrinks the weight** every update!

**Concrete example of weight decay:**

Say $w = 5.0$, $\eta = 0.1$, $\lambda = 0.01$, gradient = 0

Without regularization:

$$w_{new} = 5.0 - 0.1 \times 0 = 5.0$$ (no change)

With L2 regularization:

$$w_{new} = 5.0 \times (1 - 0.1 \times 0.01) - 0 = 5.0 \times 0.999 = 4.995$$

Weight shrinks by 0.1% per update ‚Üí "weight decay"!

**Why L2 regularization helps:**

1. **Prevents large weights**:
   - Large weights ‚Üí sensitive to small input changes
   - Small weights ‚Üí more robust
   
2. **Encourages weight sharing**:
   - Prefers many small weights over few large weights
   - Example: $w_1=5, w_2=5$ is better than $w_1=10, w_2=0$
   - $5^2 + 5^2 = 50$ < $10^2 + 0^2 = 100$
   
3. **Smoother decision boundaries**:
   - Smooth functions have small derivatives (weights)
   - Prevents fitting noise in training data

### 3.3 L1 Regularization (Lasso)

$$R(w) = \sum_{i} |w_i| = ||w||_1$$

**Breaking down EVERY symbol:**

- $R(w)$ = **L1 regularization term**
  - "L1" because it uses L1 norm (Manhattan distance)
  - Also called "Lasso" (Least Absolute Shrinkage and Selection Operator)
  
- $=$ = **Equals**

- $\sum_i$ = **Sum over all weights**
  - Goes through every weight in the network
  
- $|w_i|$ = **Absolute value** of weight
  - If $w_i = 3.0$ ‚Üí $|w_i| = 3.0$
  - If $w_i = -3.0$ ‚Üí $|w_i| = 3.0$
  - Always positive
  - Linear growth (not quadratic like L2!)
  
- $||w||_1$ = **L1 norm** (alternative notation)
  - Sum of absolute values
  - Also called "Manhattan distance" or "taxicab distance"

**Concrete numerical example:**

Same weights as before: $w = [2.0, -1.5, 0.5, -3.0]$

L1 regularization term:

$$R(w) = |2.0| + |-1.5| + |0.5| + |-3.0|$$

$$R(w) = 2.0 + 1.5 + 0.5 + 3.0 = 7.0$$

Compare to L2: $R(w) = 7.75$ (L1 is smaller for these weights)

**Gradient of L1 regularization:**

$$\frac{\partial}{\partial w_i} (\lambda |w_i|) = \lambda \cdot \text{sign}(w_i)$$

Where $\text{sign}(w_i) = \begin{cases} +1 & \text{if } w_i > 0 \\ -1 & \text{if } w_i < 0 \\ 0 & \text{if } w_i = 0 \end{cases}$

**What this means:**

The gradient is **constant** (¬±$\lambda$), not proportional to weight size!

Weight update with L1:

$$w_i := w_i - \eta(\frac{\partial L_{data}}{\partial w_i} + \lambda \cdot \text{sign}(w_i))$$

**Concrete example:**

Say $w = 5.0$, $\eta = 0.1$, $\lambda = 0.01$, gradient = 0

With L1 regularization:

$$w_{new} = 5.0 - 0.1 \times (0 + 0.01 \times 1) = 5.0 - 0.001 = 4.999$$

Weight shrinks by **fixed amount** (0.001), not percentage!

**Key difference: L1 vs L2**

For large weight ($w = 10$):
- L2: Shrinks by $\eta \lambda w = 0.1 \times 0.01 \times 10 = 0.01$ (large)
- L1: Shrinks by $\eta \lambda = 0.1 \times 0.01 = 0.001$ (fixed)

For small weight ($w = 0.1$):
- L2: Shrinks by $0.1 \times 0.01 \times 0.1 = 0.0001$ (tiny)
- L1: Shrinks by $0.1 \times 0.01 = 0.001$ (same as before!)

**Result: L1 drives small weights to exactly zero ‚Üí Sparsity!**

### 3.4 L1 vs L2: When to Use Which?

| Aspect | L2 (Ridge) | L1 (Lasso) |
|--------|-----------|-----------|
| **Penalty** | Squared weights: $w^2$ | Absolute weights: $\|w\|$ |
| **Effect on large weights** | Heavy penalty (quadratic) | Moderate penalty (linear) |
| **Effect on small weights** | Light penalty | Same penalty as large weights |
| **Sparsity** | No (weights near zero) | Yes (many weights = 0) |
| **Feature selection** | No | Yes (zeros out features) |
| **Solution** | Smooth, all weights small | Sparse, some weights = 0 |
| **Gradient** | Proportional to weight | Constant (¬±$\lambda$) |
| **Use case** | Default choice | When you need feature selection |
| **Computational** | Closed-form solution | Requires iterative solver |

**Visual comparison:**

```
L2 penalty grows quadratically:
Penalty
   ‚îÇ         ‚ï±
   ‚îÇ       ‚ï±
   ‚îÇ     ‚ï±
   ‚îÇ   ‚ï±
   ‚îÇ ‚ï±
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí |w|
   
L1 penalty grows linearly:
Penalty
   ‚îÇ       ‚ï±
   ‚îÇ     ‚ï±
   ‚îÇ   ‚ï±
   ‚îÇ ‚ï±
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí |w|
```

**Geometric intuition:**

In 2D weight space ($w_1, w_2$):

L2 constraint: $w_1^2 + w_2^2 \leq C$ (circle)
- Weights can be non-zero in any direction
- Smooth optimization

L1 constraint: $|w_1| + |w_2| \leq C$ (diamond)
- Diamond has corners on axes ($w_1=0$ or $w_2=0$)
- Optimization likely hits corners ‚Üí sparsity!

**Practical recommendation:**

- **Start with L2** (default choice, works 95% of time)
- **Use L1** when:
  - You have many features and want automatic selection
  - You need interpretable models (which features matter?)
  - You want to reduce model size (sparse networks)

### 3.5 Elastic Net (L1 + L2)

Combines both penalties:

$$R(w) = \lambda_1 \sum_i |w_i| + \lambda_2 \sum_i w_i^2$$

**Breaking down:**

- $\lambda_1$ = L1 regularization strength
- $\lambda_2$ = L2 regularization strength
- Gets benefits of both: sparsity + smoothness

**When to use:**
- High-dimensional data with correlated features
- Want some sparsity but also stability of L2
- Have time to tune two hyperparameters

---

## 4. Dropout

### 4.1 The Core Idea

**Randomly "drop" (set to zero) neurons during training**

Invented by Hinton et al. (2012) - revolutionized deep learning!

**Algorithm:**

During training (forward pass):
1. For each neuron in a layer
2. With probability $p$ (dropout rate), set output to 0
3. With probability $(1-p)$, keep the neuron active
4. Scale remaining activations by $\frac{1}{1-p}$

During testing:
- **Use all neurons** (no dropout)
- No scaling needed (due to training-time scaling)

**Mathematical formulation:**

Training:

$$a^{[l]} = \text{activation}(z^{[l]}) \odot m^{[l]} \cdot \frac{1}{1-p}$$

**Breaking down EVERY symbol:**

- $a^{[l]}$ = **Activation output** from layer l with dropout
  - This is what gets passed to next layer
  
- $=$ = **Assignment**

- $\text{activation}(z^{[l]})$ = **Regular activation** (ReLU, sigmoid, etc.)
  - $z^{[l]}$ = pre-activation (weighted sum)
  - This is computed normally first
  
- $\odot$ = **Element-wise multiplication** (Hadamard product)
  - Multiply corresponding elements
  - Example: $[1, 2, 3] \odot [1, 0, 1] = [1, 0, 3]$
  
- $m^{[l]}$ = **Dropout mask** (binary random vector)
  - Shape: Same as activation
  - Each element is 0 or 1
  - Generated randomly for each forward pass!
  - $m_i = \begin{cases} 0 & \text{with probability } p \text{ (drop)} \\ 1 & \text{with probability } 1-p \text{ (keep)} \end{cases}$
  
- $\cdot \frac{1}{1-p}$ = **Inverted dropout scaling**
  - Compensates for dropped neurons
  - If $p = 0.5$ ‚Üí scale by $\frac{1}{0.5} = 2$
  - Ensures expected value stays the same

**Why the scaling factor?**

Without scaling:
- Training: Average activation = $(1-p) \times a$ (some neurons dropped)
- Testing: Average activation = $a$ (all neurons active)
- Mismatch! Network behaves differently!

With scaling by $\frac{1}{1-p}$:
- Training: Average = $(1-p) \times a \times \frac{1}{1-p} = a$ ‚úì
- Testing: Average = $a$ ‚úì
- Consistent!

**Concrete numerical example:**

Say we have activations: $a = [1.0, 2.0, 3.0, 4.0]$, $p = 0.5$ (drop 50%)

Step 1: Generate random mask
- $m = [1, 0, 1, 0]$ (randomly dropped 2nd and 4th neurons)

Step 2: Apply mask
- $a \odot m = [1.0, 0, 3.0, 0]$

Step 3: Scale by $\frac{1}{1-p} = \frac{1}{0.5} = 2$
- $a_{final} = [2.0, 0, 6.0, 0]$

Original sum: $1+2+3+4 = 10$
After dropout: $2+0+6+0 = 8$ (close to 10 on average!)

### 4.2 Why Dropout Works

**Multiple explanations:**

**1. Ensemble of networks:**
- Each dropout pattern creates a different "sub-network"
- With $n$ neurons, there are $2^n$ possible sub-networks!
- Training with dropout = training ensemble of $2^n$ networks
- At test time: averaging predictions (model averaging)
- Ensembles always generalize better

**2. Reduces co-adaptation:**
- Without dropout: Neurons can "collaborate" too much
  - Neuron A learns to detect "eyes"
  - Neuron B learns "face ONLY if neuron A fires"
  - Too specialized, fragile!
- With dropout: Neuron B can't rely on neuron A (might be dropped!)
  - Each neuron must learn useful features independently
  - More robust representations

**3. Implicit data augmentation:**
- Each forward pass sees a different sub-network
- It's like training on different "versions" of the data
- Adds noise ‚Üí forces learning of robust features

**4. Bayesian interpretation:**
- Dropout ‚âà approximate Bayesian inference
- Each sub-network is a sample from posterior distribution
- Captures model uncertainty

### 4.3 Dropout Hyperparameters

**Dropout rate ($p$):**

| Layer Type | Typical $p$ | Reason |
|-----------|-------------|--------|
| Input layer | 0.0 - 0.2 | Don't drop too much raw data |
| Hidden layers | 0.3 - 0.5 | Standard choice |
| Large hidden layers | 0.5 - 0.7 | More neurons ‚Üí can drop more |
| Output layer | 0.0 | Never drop outputs! |

**Common values:**
- $p = 0.5$ ‚Üí Drop 50% of neurons (most popular)
- $p = 0.3$ ‚Üí Drop 30% (conservative)
- $p = 0.7$ ‚Üí Drop 70% (aggressive, for very wide networks)

**Tuning strategy:**
1. Start with $p = 0.5$ for hidden layers
2. If still overfitting ‚Üí increase to 0.6 or 0.7
3. If underfitting ‚Üí decrease to 0.3 or remove dropout

### 4.4 Implementing Dropout

**Training mode:**

```python
# Forward pass with dropout
def forward_with_dropout(X, W, p=0.5):
    # Compute activations normally
    a = relu(X @ W)  # Shape: (batch_size, neurons)
    
    # Generate dropout mask
    # Each element is 1 with probability (1-p), else 0
    mask = np.random.binomial(1, 1-p, size=a.shape)
    
    # Apply mask and scale
    a_dropout = a * mask / (1-p)
    
    # Store mask for backward pass
    return a_dropout, mask
```

**Test mode:**

```python
# Forward pass at test time (no dropout)
def forward_test(X, W):
    a = relu(X @ W)
    # Use all neurons, no scaling needed!
    return a
```

**Backward pass:**

The gradient only flows through neurons that weren't dropped:

```python
def backward_with_dropout(da, mask, p=0.5):
    # Gradient only flows through active neurons
    # Apply same mask and scaling
    da_prev = da * mask / (1-p)
    return da_prev
```

**Implementation note:** In modern frameworks (PyTorch, TensorFlow):

```python
# PyTorch
self.dropout = nn.Dropout(p=0.5)

# Training mode
self.train()  # Enables dropout
output = self.dropout(x)

# Test mode
self.eval()  # Disables dropout
output = self.dropout(x)  # No dropout applied
```

### 4.5 Where to Apply Dropout

**Recommended:**

1. **Fully connected (dense) layers**
   - Most common use case
   - Apply after activation function
   - Typical: $p = 0.5$

2. **Between CNN layers (use sparingly)**
   - CNNs already have strong spatial regularization
   - If needed: $p = 0.1$ to $0.3$
   - Modern practice: Use batch norm instead

3. **After RNN/LSTM outputs**
   - NOT on recurrent connections (breaks temporal dynamics)
   - Apply to output projections only
   - Typical: $p = 0.3$ to $0.5$

**NOT recommended:**

- ‚ùå Input layer (use $p \leq 0.2$ if needed)
- ‚ùå Output layer (never!)
- ‚ùå Batch norm layers (they regularize differently)
- ‚ùå Very small networks (dropout needs redundancy)

### 4.6 Dropout Variations

**DropConnect:**
- Instead of dropping neurons, drop individual **weights**
- $W_{dropout} = W \odot M$ where $M$ is binary mask
- More fine-grained than dropout

**Spatial Dropout (2D):**
- For CNNs: Drop entire feature maps, not individual pixels
- Preserves spatial structure
- Better for convolutional layers

**Concrete Dropout:**
- Learns optimal dropout rate automatically
- Uses Bayesian optimization
- More principled but complex

---

## 5. Batch Normalization

### 5.1 The Internal Covariate Shift Problem

**The problem:** During training, layer inputs' distributions keep changing!

Imagine Layer 2 in a network:
- Epoch 1: Receives inputs in range [-1, 1]
- Epoch 10: Receives inputs in range [-5, 5] (due to Layer 1's weight updates)
- Epoch 20: Receives inputs in range [-0.1, 0.1]

Each layer must constantly adapt to changing input distributions!

**Consequences:**
1. **Slow training**: Layers keep "chasing" moving targets
2. **Requires small learning rates**: Prevent instability
3. **Sensitive to initialization**: Bad init ‚Üí bad distributions ‚Üí slow learning
4. **Vanishing/exploding gradients**: Distributions can go to extremes

**Solution: Batch Normalization (Ioffe & Szegedy, 2015)**

Normalize layer inputs to have **consistent** mean and variance!

### 5.2 Batch Normalization Algorithm

**Applied to mini-batch of activations** $X = \{x_1, x_2, ..., x_m\}$

**Step 1: Compute batch statistics**

$$\mu_B = \frac{1}{m} \sum_{i=1}^{m} x_i$$

**Breaking down:**

- $\mu_B$ = **Batch mean** (Greek letter "mu")
  - Average of all activations in the batch
  - $B$ subscript = "batch"
  - Computed per feature dimension
  
- $\frac{1}{m}$ = **Average** over batch size $m$

- $\sum_{i=1}^{m} x_i$ = **Sum** of all samples in batch
  - $i$ = sample index (1, 2, ..., m)

$$\sigma_B^2 = \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu_B)^2$$

**Breaking down:**

- $\sigma_B^2$ = **Batch variance** (Greek letter "sigma" squared)
  - Measures spread of activations
  - $^2$ = squared (variance, not standard deviation)
  
- $(x_i - \mu_B)^2$ = **Squared deviation** from mean
  - How far is sample $i$ from average?
  - Squared to make positive
  - Large variance ‚Üí values spread out

**Step 2: Normalize**

$$\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$

**Breaking down EVERY symbol:**

- $\hat{x}_i$ = **Normalized activation** for sample $i$
  - Hat (^) indicates normalized
  - This is what we pass to next layer (after step 3)
  
- $=$ = **Equals**

- $x_i - \mu_B$ = **Center** the activation (subtract mean)
  - Shifts distribution to have mean = 0
  - If $\mu_B = 5$ and $x_i = 8$ ‚Üí $x_i - \mu_B = 3$
  
- $\div$ = **Division**

- $\sqrt{\sigma_B^2 + \epsilon}$ = **Standard deviation** (with stability term)
  - $\sqrt{\sigma_B^2}$ = $\sigma_B$ = standard deviation
  - $\epsilon$ = tiny constant (1e-5) to prevent division by zero
  - If $\sigma_B = 2$ and $\sigma_B^2 = 4$, then $\sqrt{4 + 10^{-5}} \approx 2$
  
After this step: $\hat{x}_i$ has mean ‚âà 0, variance ‚âà 1 (standard normal!)

**Step 3: Scale and shift**

$$y_i = \gamma \hat{x}_i + \beta$$

**Breaking down EVERY symbol:**

- $y_i$ = **Final output** after batch norm
  - This is what actually gets passed to next layer
  - No hat (^) = not normalized anymore!
  
- $=$ = **Equals**

- $\gamma$ = **Scale parameter** (Greek letter "gamma")
  - **Learnable parameter** (trained via backprop!)
  - Allows network to "undo" normalization if needed
  - Initialized to 1
  - Shape: (num_features,) - one per feature dimension
  
- $\hat{x}_i$ = **Normalized activation** from step 2

- $+$ = **Addition**

- $\beta$ = **Shift parameter** (Greek letter "beta")
  - **Learnable parameter** (trained via backprop!)
  - Allows network to shift mean if needed
  - Initialized to 0
  - Shape: (num_features,) - one per feature dimension

**Why scale and shift?**

Normalization forces mean=0, var=1, but maybe the network **wants** different values!

- If optimal mean is 5 and variance is 4:
  - Network can learn $\gamma = 2$ and $\beta = 5$
  - Then $y = 2 \hat{x} + 5$ recovers desired distribution
  
- Special case: $\gamma = \sigma_B$ and $\beta = \mu_B$ ‚Üí recovers original input!
  - Network can "undo" batch norm if it's not helpful

### 5.3 Concrete Numerical Example

Say we have batch of 4 samples (after activation):

$$X = \begin{bmatrix} 1 \\ 3 \\ 5 \\ 7 \end{bmatrix}$$

**Step 1: Compute batch statistics**

Batch mean:

$$\mu_B = \frac{1}{4}(1 + 3 + 5 + 7) = \frac{16}{4} = 4$$

Batch variance:

$$\sigma_B^2 = \frac{1}{4}[(1-4)^2 + (3-4)^2 + (5-4)^2 + (7-4)^2]$$

$$= \frac{1}{4}[9 + 1 + 1 + 9] = \frac{20}{4} = 5$$

Standard deviation:

$$\sigma_B = \sqrt{5 + 10^{-5}} \approx 2.236$$

**Step 2: Normalize**

$$\hat{x}_1 = \frac{1 - 4}{2.236} = \frac{-3}{2.236} = -1.342$$

$$\hat{x}_2 = \frac{3 - 4}{2.236} = \frac{-1}{2.236} = -0.447$$

$$\hat{x}_3 = \frac{5 - 4}{2.236} = \frac{1}{2.236} = 0.447$$

$$\hat{x}_4 = \frac{7 - 4}{2.236} = \frac{3}{2.236} = 1.342$$

Check: $\hat{X} = [-1.342, -0.447, 0.447, 1.342]$
- Mean ‚âà 0 ‚úì
- Variance ‚âà 1 ‚úì

**Step 3: Scale and shift**

Say $\gamma = 2.0$ and $\beta = 1.0$ (learned parameters):

$$y_1 = 2.0 \times (-1.342) + 1.0 = -2.684 + 1.0 = -1.684$$

$$y_2 = 2.0 \times (-0.447) + 1.0 = -0.894 + 1.0 = 0.106$$

$$y_3 = 2.0 \times (0.447) + 1.0 = 0.894 + 1.0 = 1.894$$

$$y_4 = 2.0 \times (1.342) + 1.0 = 2.684 + 1.0 = 3.684$$

Final output: $Y = [-1.684, 0.106, 1.894, 3.684]$

This output has mean ‚âà 1 and std ‚âà 2 (controlled by $\beta$ and $\gamma$!)

### 5.4 Batch Norm at Test Time

**Problem:** At test time, we might have single sample (batch size = 1)!

Can't compute meaningful mean/variance from one sample.

**Solution:** Use **running statistics** from training:

During training:
- Keep **exponential moving average** of batch means and variances
- $\mu_{running} = \alpha \mu_{running} + (1-\alpha) \mu_B$
- $\sigma_{running}^2 = \alpha \sigma_{running}^2 + (1-\alpha) \sigma_B^2$
- Typical $\alpha = 0.9$ or $0.99$

At test time:
- Use $\mu_{running}$ and $\sigma_{running}$ instead of batch statistics
- Normalize: $\hat{x} = \frac{x - \mu_{running}}{\sqrt{\sigma_{running}^2 + \epsilon}}$
- Scale and shift: $y = \gamma \hat{x} + \beta$

### 5.5 Why Batch Normalization Works

**Multiple benefits:**

**1. Reduces internal covariate shift:**
- Layer inputs have consistent distributions
- Each layer trains faster (doesn't chase moving target)

**2. Acts as regularization:**
- Batch statistics add noise (different for each batch)
- Similar to dropout but less aggressive
- Can reduce/remove dropout when using batch norm

**3. Allows higher learning rates:**
- Normalization prevents exploding activations/gradients
- Can use 10-100x larger learning rates
- Faster convergence!

**4. Reduces sensitivity to initialization:**
- Even with bad init, batch norm normalizes activations
- Xavier/He initialization less critical

**5. Smoother optimization landscape:**
- Loss function becomes more "well-behaved"
- Fewer sharp minima, easier to optimize

**Empirical results:**
- ImageNet training: 14x faster convergence
- Can achieve same accuracy with fewer epochs
- Enabled training of very deep networks (100+ layers)

### 5.6 Where to Apply Batch Norm

**Standard placement:**

```
Linear ‚Üí BatchNorm ‚Üí Activation ‚Üí (Dropout)
```

or

```
Linear ‚Üí Activation ‚Üí BatchNorm ‚Üí (Dropout)
```

**Both work, but convention:**
- **Before activation** (original paper)
  - Normalize the pre-activations $z$
  - Most common in practice
  
- **After activation** (also works)
  - Normalize the post-activations $a$
  - Some empirical advantages in certain cases

**Full network example:**

```
Input (784)
   ‚Üì
Dense(784 ‚Üí 256)
   ‚Üì
BatchNorm(256)  ‚Üê Normalize!
   ‚Üì
ReLU
   ‚Üì
Dense(256 ‚Üí 128)
   ‚Üì
BatchNorm(128)  ‚Üê Normalize!
   ‚Üì
ReLU
   ‚Üì
Dense(128 ‚Üí 10)
   ‚Üì
Softmax (NO batch norm on output!)
```

**Important notes:**

- ‚ùå Don't use on output layer
- ‚ùå Don't use with very small batch sizes (<8)
- ‚úÖ Use with medium/large batch sizes (32+)
- ‚úÖ Especially helpful in deep networks (5+ layers)

### 5.7 Batch Norm Hyperparameters

**Momentum ($\alpha$):**
- For running statistics: $\mu_{running} = \alpha \mu_{running} + (1-\alpha) \mu_B$
- Typical: 0.9 or 0.99
- Higher = more history retained

**Epsilon ($\epsilon$):**
- Numerical stability in normalization
- Typical: 1e-5 or 1e-8
- Prevents division by zero

**These are usually left at defaults - no tuning needed!**

---

## 6. Early Stopping

### 6.1 The Concept

**Stop training when validation performance stops improving**

Simple but effective regularization technique!

**Algorithm:**

1. Train model and track validation loss/accuracy
2. Save model checkpoint when validation improves
3. If validation doesn't improve for $n$ epochs (patience), stop training
4. Restore best checkpoint

**Visual:**

```
Val Loss
  ‚îÇ
  ‚îÇ  ‚óè                     ‚Üê Best model (epoch 15)
  ‚îÇ   ‚óè
  ‚îÇ    ‚óè‚óè
  ‚îÇ      ‚óè‚óè‚óè
  ‚îÇ         ‚óè‚óè‚óè‚óè‚óè   ‚Üê Stop here (epoch 25)
  ‚îÇ              ‚óè‚óè‚óè  Patience = 10 epochs
  ‚îÇ                 ‚óè
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Epoch
  1        15      25    30

Training stops at epoch 25, but we restore model from epoch 15!
```

### 6.2 Implementation

**Pseudocode:**

```python
best_val_loss = infinity
patience = 10
counter = 0
best_model = None

for epoch in range(max_epochs):
    train_loss = train_one_epoch()
    val_loss = validate()
    
    # Check if validation improved
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_model = copy_model()  # Save checkpoint
        counter = 0  # Reset patience counter
    else:
        counter += 1  # No improvement
    
    # Check if patience exceeded
    if counter >= patience:
        print(f"Early stopping at epoch {epoch}")
        break

# Restore best model
model = best_model
```

### 6.3 Early Stopping Hyperparameters

**Patience:**
- How many epochs to wait for improvement
- **Too small** (5): Might stop too early, underfit
- **Too large** (100): Wastes time, overfits
- **Typical values:** 10-20 for small datasets, 30-50 for large

**Min delta:**
- Minimum improvement to count as "better"
- Prevents stopping due to tiny fluctuations
- Typical: 0.001 or 0.0001
- Example: Only count as improvement if val_loss decreases by >0.001

**Metric to monitor:**
- Validation loss (most common)
- Validation accuracy (for classification)
- Custom metrics (F1, AUC, etc.)

### 6.4 Advantages and Disadvantages

**Advantages:**

‚úÖ **Simple**: Easy to implement
‚úÖ **No hyperparameters**: Only patience (vs L2 Œª, dropout p)
‚úÖ **Automatic**: Determines when to stop
‚úÖ **Efficient**: Doesn't waste time overtraining
‚úÖ **Often works**: Effective regularization

**Disadvantages:**

‚ùå **Requires validation set**: Needs held-out data
‚ùå **Noisy validation**: Might stop too early due to noise
‚ùå **Longer training**: Must train until overfitting appears
‚ùå **Not suitable for small datasets**: High variance in val metrics

**Best practice:** Combine with other regularization!
- L2 + Early Stopping
- Dropout + Batch Norm + Early Stopping

---

## 7. Data Augmentation

### 7.1 The Concept

**Artificially expand training data** by applying transformations

Core idea: Create "new" training samples from existing ones!

**For images:**
- Flip horizontally
- Rotate ¬±15¬∞
- Zoom in/out
- Add noise
- Adjust brightness/contrast
- Random crops

**Example:**

Original cat image ‚Üí 10 augmented versions:
1. Original
2. Flipped horizontally
3. Rotated 10¬∞ clockwise
4. Rotated 10¬∞ counter-clockwise
5. Zoomed in 10%
6. Brightness +20%
7. Brightness -20%
8. Random crop (top-left)
9. Random crop (bottom-right)
10. Combination of above

Effective dataset size: 10x larger!

### 7.2 Image Augmentation Techniques

**Geometric transformations:**

1. **Horizontal flip**
   - Mirror image left-right
   - Use case: Objects look same flipped (cats, cars)
   - Don't use: Text, digits (3 ‚â† ∆ê)

2. **Rotation**
   - Rotate ¬±15¬∞ to ¬±45¬∞
   - Use case: Object orientation doesn't matter
   - Don't use: Upright objects (people, buildings)

3. **Translation**
   - Shift image left/right/up/down
   - Simulates different camera positions

4. **Scaling/Zoom**
   - Zoom in (crop) or out (pad)
   - Simulates different distances

5. **Shearing**
   - Skew image perspective
   - Simulates viewing angle changes

**Color transformations:**

1. **Brightness**
   - Multiply pixels by constant: $x_{new} = \alpha x$
   - Simulates different lighting

2. **Contrast**
   - Stretch/compress pixel range
   - $x_{new} = 128 + \alpha(x - 128)$

3. **Saturation**
   - Adjust color intensity
   - Grayscale ‚Üî Vivid colors

4. **Hue shift**
   - Rotate colors (red ‚Üí green ‚Üí blue ‚Üí red)

**Noise and corruption:**

1. **Gaussian noise**
   - Add random noise: $x_{new} = x + \mathcal{N}(0, \sigma^2)$

2. **Salt and pepper**
   - Random black/white pixels

3. **Cutout/Random erasing**
   - Black out random rectangular regions
   - Forces network to use context

### 7.3 Implementation Example (PyTorch)

```python
import torchvision.transforms as transforms

# Define augmentation pipeline
train_transform = transforms.Compose([
    # Resize to standard size
    transforms.Resize((224, 224)),
    
    # Random augmentations
    transforms.RandomHorizontalFlip(p=0.5),  # 50% chance flip
    transforms.RandomRotation(degrees=15),    # ¬±15¬∞ rotation
    transforms.ColorJitter(
        brightness=0.2,  # ¬±20% brightness
        contrast=0.2,    # ¬±20% contrast
        saturation=0.2,  # ¬±20% saturation
        hue=0.1          # ¬±10% hue
    ),
    transforms.RandomCrop(224, padding=4),   # Random crop
    
    # Convert to tensor and normalize
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],  # ImageNet statistics
        std=[0.229, 0.224, 0.225]
    )
])

# For test/validation: NO augmentation (except resize/normalize)
test_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])
```

### 7.4 Domain-Specific Augmentation

**Computer vision:**
- All techniques above
- Mixup: Blend two images
- CutMix: Paste crop from one image onto another

**Natural language processing:**
- Synonym replacement
- Random insertion/deletion
- Back-translation (translate to French, then back to English)
- Word embedding perturbation

**Audio:**
- Time stretching
- Pitch shifting
- Add background noise
- Change speed

**Time series:**
- Time warping
- Magnitude warping
- Jittering

### 7.5 How Much Augmentation?

**Guidelines:**

**Light augmentation (good starting point):**
- Horizontal flip: 50%
- Rotation: ¬±10¬∞
- Brightness/contrast: ¬±10%

**Medium augmentation (standard):**
- Horizontal flip: 50%
- Rotation: ¬±15¬∞
- Zoom: 0.9-1.1x
- Brightness/contrast: ¬±20%

**Heavy augmentation (aggressive):**
- All geometric transforms
- All color transforms
- Cutout/mixup
- Risk: Too much ‚Üí hurts performance

**Rule of thumb:**
- Small dataset (<10k samples) ‚Üí Aggressive augmentation
- Large dataset (>100k samples) ‚Üí Light augmentation
- If overfitting ‚Üí Increase augmentation
- If underfitting ‚Üí Decrease augmentation

---

## 8. Other Regularization Techniques

### 8.1 Label Smoothing

Instead of hard targets (0 or 1), use soft targets:

Hard: $y = [0, 0, 1, 0]$ (class 2)
Soft: $y = [0.025, 0.025, 0.925, 0.025]$ (mostly class 2)

**Formula:**

$$y_{smooth} = (1 - \epsilon) y + \frac{\epsilon}{K}$$

Where:
- $\epsilon$ = smoothing parameter (typically 0.1)
- $K$ = number of classes
- Prevents overconfidence

### 8.2 Mixup

Train on **blended images and labels**:

$$x_{mix} = \lambda x_i + (1-\lambda) x_j$$

$$y_{mix} = \lambda y_i + (1-\lambda) y_j$$

Where $\lambda \sim \text{Beta}(\alpha, \alpha)$, typically $\alpha = 0.2$

Forces smooth decision boundaries!

### 8.3 Gradient Clipping

**Prevents exploding gradients** by capping gradient magnitude:

$$g_{clipped} = \begin{cases}
g & \text{if } ||g|| \leq \theta \\
\frac{\theta \cdot g}{||g||} & \text{if } ||g|| > \theta
\end{cases}$$

Where $\theta$ is the threshold (e.g., 1.0 or 5.0)

### 8.4 Weight Constraints

**Max norm constraint:**

After each update, if $||w|| > c$, rescale: $w := \frac{c \cdot w}{||w||}$

Commonly used with dropout in RNNs.

---

## 9. Practical Guidelines

### 9.1 Regularization Strategy

**Step-by-step approach:**

**Step 1: Establish baseline**
- Train without regularization
- Observe: overfitting? underfitting?

**Step 2: If overfitting (train >> val):**
1. Add L2 regularization (Œª = 0.001)
2. Add dropout (p = 0.5) to large layers
3. Add batch normalization
4. Implement early stopping (patience = 10-20)
5. Use data augmentation (if applicable)

**Step 3: If underfitting (both train and val poor):**
- Make model larger (more layers/neurons)
- Train longer
- Reduce regularization
- Check learning rate (might be too small)

**Step 4: Fine-tune hyperparameters:**
- Adjust L2 Œª: try [0.0001, 0.001, 0.01]
- Adjust dropout p: try [0.3, 0.5, 0.7]
- Adjust early stopping patience

### 9.2 Which Regularization to Use?

**Default recipe (works 90% of time):**

```python
model = Sequential([
    Dense(512),
    BatchNorm(),      # ‚Üê Almost always helps
    ReLU(),
    Dropout(0.5),     # ‚Üê If overfitting
    
    Dense(256),
    BatchNorm(),
    ReLU(),
    Dropout(0.5),
    
    Dense(num_classes)
])

optimizer = Adam(lr=0.001, weight_decay=0.0001)  # ‚Üê L2 regularization

# During training
use_data_augmentation()  # ‚Üê If images
use_early_stopping(patience=20)
```

**Prioritization:**

1. **Batch Normalization** - Always try first
2. **Data Augmentation** - Free performance boost (images)
3. **L2 Regularization** - Simple, effective
4. **Dropout** - If still overfitting after above
5. **Early Stopping** - Safety net

### 9.3 Hyperparameter Tuning

**L2 regularization (Œª):**
- Start: 0.001
- Too much overfitting ‚Üí increase to 0.01
- Underfitting ‚Üí decrease to 0.0001 or remove

**Dropout rate (p):**
- Start: 0.5 (drop 50%)
- Still overfitting ‚Üí increase to 0.6-0.7
- Underfitting ‚Üí decrease to 0.3 or remove

**Batch norm momentum:**
- Default: 0.9 or 0.99
- Usually no need to change

**Early stopping patience:**
- Small dataset: 10-20 epochs
- Large dataset: 20-50 epochs
- Noisy validation: Increase patience

### 9.4 Common Mistakes

**‚ùå Mistake 1: Too much regularization**
- Symptom: Poor training AND validation accuracy
- Solution: Reduce regularization strength

**‚ùå Mistake 2: Regularizing the output layer**
- Never use dropout on output neurons!
- Biases can be regularized or not (convention: not)

**‚ùå Mistake 3: Using batch norm with small batches**
- Batch size < 8 ‚Üí unreliable statistics
- Solution: Use larger batches or remove batch norm

**‚ùå Mistake 4: Forgetting to disable augmentation at test time**
- Augmentation is for training only!
- Test images should be clean

**‚ùå Mistake 5: Not using early stopping**
- Wastes compute by overtraining
- Always monitor validation loss!

---

## 10. Implementation Strategy

### 10.1 What We'll Implement

**File 1: `regularization_techniques.py`**

1. **Dense layer with L1/L2 regularization**
   - Forward pass with weight decay
   - Backward pass with regularization gradients
   
2. **Dropout layer**
   - Training mode (random dropout + scaling)
   - Test mode (no dropout)
   - Inverted dropout implementation
   
3. **Batch Normalization layer**
   - Forward pass (normalize + scale/shift)
   - Backward pass (complex gradients!)
   - Running statistics for test time
   
4. **Regularized neural network class**
   - Combines all techniques
   - Training mode vs test mode
   - Early stopping logic

**File 2: `project_overfit_detector.py`**

Build diagnostic tool that:
1. Trains model with/without regularization
2. Plots train vs val loss/accuracy
3. Detects overfitting automatically
4. Suggests which regularization to add
5. Visualizes effect of different techniques

### 10.2 Design Philosophy

**Principles:**
- Clear separation: train mode vs test mode
- Extensive comments explaining math
- Visualizations for intuition
- Comparison plots (with/without regularization)

---

## üìù Summary

### What You'll Learn:

1. ‚úÖ **Overfitting** = memorizing training data, failing on test data
2. ‚úÖ **L1 regularization** = sparse weights, feature selection
3. ‚úÖ **L2 regularization** = small weights, smoother functions
4. ‚úÖ **Dropout** = randomly drop neurons, ensemble effect
5. ‚úÖ **Batch Normalization** = normalize layer inputs, faster training
6. ‚úÖ **Early Stopping** = stop when validation stops improving
7. ‚úÖ **Data Augmentation** = artificially expand dataset

### Key Takeaways:

- **Always split data**: Train / Validation / Test
- **Monitor both**: Training AND validation metrics
- **Regularize**: Especially with deep networks
- **Start simple**: Add regularization incrementally
- **Batch norm first**: Usually the biggest win
- **Tune carefully**: Too much regularization ‚Üí underfitting

### Next Chapter:

**Chapter 05: Convolutional Neural Networks**
- Learn how CNNs process images
- Implement convolution and pooling from scratch
- Build CIFAR-10 classifier (80%+ accuracy)
- Understand why CNNs dominate computer vision

---

**Ready to implement? Let's build regularization from scratch!** üöÄ
