# üöÄ Deep Learning Mastery: Zero to Hero Course
## Your Complete Roadmap to Landing a ‚Çπ1 Crore+ Deep Learning Role

<div align="center">

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![NumPy](https://img.shields.io/badge/NumPy-from%20scratch-green.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-production-red.svg)
![Status](https://img.shields.io/badge/Status-In%20Progress-yellow.svg)
![License](https://img.shields.io/badge/License-MIT-purple.svg)

**From Absolute Beginner ‚Üí Production-Ready Deep Learning Engineer**

*Master Deep Learning, Transformers, LLMs, Reinforcement Learning & Land Your Dream AI Job*

‚≠ê **Star this repo if you find it helpful!** ‚≠ê

</div>

---

## üéØ Course Mission

This is **THE MOST COMPREHENSIVE** deep learning course designed to take you from knowing nothing about neural networks to becoming a job-ready deep learning engineer capable of:

- ‚úÖ Building production-grade deep learning systems from scratch
- ‚úÖ Training and deploying large language models (LLMs)
- ‚úÖ Implementing reinforcement learning algorithms
- ‚úÖ Designing multimodal AI systems
- ‚úÖ Acing technical interviews at top AI companies
- ‚úÖ Landing roles with ‚Çπ1 Crore+ annual packages

### üí∞ Target Outcome: ‚Çπ1 CR+ Package

This course is specifically designed around the skills and knowledge required for **high-paying deep learning roles** at:
- **Top AI Labs**: OpenAI, Anthropic, DeepMind, Cohere
- **FAANG**: Google AI, Meta AI, Amazon Science
- **Indian AI Unicorns**: Ola Electric (AI), Nykaa (AI/ML), Fractal Analytics
- **Global AI Companies**: Nvidia, Microsoft Research, Hugging Face
- **Startups**: Series B+ AI startups with strong funding

**Average salary range after completion**: ‚Çπ40L - ‚Çπ1.2 CR per year (depending on experience, interview performance, and company)

---

## üèóÔ∏è Course Architecture

### Duration: 28-39 weeks (7-10 months of intensive learning)

This course follows a **teaching-by-doing** methodology where **every single concept** is:
1. **Explained theoretically** with detailed README files (mathematical foundations, intuitions, real-world applications)
2. **Implemented from scratch** in Python with line-by-line code explanations
3. **Applied in projects** to solidify understanding and build your portfolio

---

## üìö Module Breakdown

### **Module 01: Deep Learning Fundamentals** (6-8 weeks)
**Goal**: Master the foundation of neural networks and core architectures

#### Chapters:
1. **Neural Networks from Scratch**
   - Theory: Perceptrons, activation functions, forward propagation
   - Code: Pure NumPy implementation of multilayer perceptron
   - Project: Handwritten digit classifier (MNIST)

2. **Backpropagation & Gradient Descent**
   - Theory: Chain rule, computational graphs, gradient flow
   - Code: Implementing backprop from scratch, automatic differentiation
   - Project: Custom autograd engine (mini PyTorch)

3. **Training Techniques & Optimization**
   - Theory: SGD, momentum, Adam, learning rate schedules
   - Code: All optimizers from scratch with visualizations
   - Project: Training dynamics visualizer

4. **Regularization & Generalization**
   - Theory: Overfitting, L1/L2 regularization, dropout, batch norm
   - Code: Implementing all regularization techniques
   - Project: Overfit detector and prevention system

5. **Convolutional Neural Networks (CNNs)**
   - Theory: Convolutions, pooling, receptive fields, CNN architectures
   - Code: CNN from scratch, then PyTorch implementation
   - Project: Image classifier (CIFAR-10) with data augmentation

6. **Advanced CNN Architectures**
   - Theory: ResNet, DenseNet, EfficientNet, MobileNet
   - Code: Implementing residual connections, skip connections
   - Project: Transfer learning for custom image classification

7. **Recurrent Neural Networks (RNNs)**
   - Theory: Sequential processing, hidden states, BPTT
   - Code: Vanilla RNN, LSTM, GRU from scratch
   - Project: Text generation (character-level language model)

8. **Advanced RNNs & Seq2Seq**
   - Theory: Encoder-decoder, attention in RNNs, bidirectional RNNs
   - Code: Seq2Seq with attention
   - Project: Neural machine translation (English to French)

**Module Outcome**: Strong foundation in neural networks, CNNs, and RNNs. Portfolio projects demonstrating core skills.

---

### **Module 02: Transformers & Attention Mechanisms** (4-6 weeks)
**Goal**: Master the architecture that powers modern AI (GPT, BERT, Vision Transformers)

#### Chapters:
1. **Attention Mechanism from Scratch**
   - Theory: Self-attention, query-key-value, attention scores
   - Code: Implementing scaled dot-product attention in NumPy
   - Project: Attention visualizer for sequence data

2. **Multi-Head Attention**
   - Theory: Multiple attention heads, parallel processing
   - Code: Multi-head attention from scratch
   - Project: Sentiment analysis with attention weights

3. **Transformer Architecture**
   - Theory: Encoder, decoder, positional encoding, layer norm
   - Code: Full transformer implementation from scratch
   - Project: Language translation with custom transformer

4. **BERT & Masked Language Modeling**
   - Theory: Bidirectional encoding, masked LM, next sentence prediction
   - Code: Pre-training BERT from scratch (simplified version)
   - Project: Text classification with fine-tuned BERT

5. **GPT & Autoregressive Models**
   - Theory: Decoder-only architecture, causal masking, autoregressive generation
   - Code: Mini-GPT implementation from scratch
   - Project: Code generation model (Python code completion)

6. **Vision Transformers (ViT)**
   - Theory: Patch embeddings, applying transformers to images
   - Code: ViT from scratch in PyTorch
   - Project: Image classification with ViT (ImageNet subset)

**Module Outcome**: Deep understanding of transformers, the backbone of modern AI. Ability to implement and modify transformer architectures.

---

### **Module 03: Large Language Models (LLMs)** (6-8 weeks)
**Goal**: Learn to build, train, fine-tune, and deploy production LLMs

#### Chapters:
1. **LLM Architecture Deep Dive**
   - Theory: GPT-2, GPT-3, LLaMA architecture, scaling laws
   - Code: Implementing GPT-2 from scratch
   - Project: Train mini-LLM on custom text corpus

2. **Tokenization & Embeddings**
   - Theory: BPE, WordPiece, SentencePiece, embedding layers
   - Code: Building custom tokenizer from scratch
   - Project: Multi-lingual tokenizer for Indian languages

3. **Pre-training LLMs**
   - Theory: Next-token prediction, distributed training, mixed precision
   - Code: Pre-training pipeline with PyTorch DDP
   - Project: Pre-train 125M parameter model on Wikipedia

4. **Fine-tuning Techniques**
   - Theory: Full fine-tuning, LoRA, QLoRA, adapter layers
   - Code: Implementing LoRA from scratch, PEFT library
   - Project: Fine-tune LLaMA for domain-specific tasks

5. **Instruction Tuning & RLHF**
   - Theory: Supervised fine-tuning (SFT), reward modeling, PPO for LLMs
   - Code: RLHF pipeline implementation
   - Project: Build instruction-following chatbot

6. **Prompt Engineering & In-Context Learning**
   - Theory: Zero-shot, few-shot, chain-of-thought prompting
   - Code: Advanced prompting techniques, prompt optimization
   - Project: Prompt engineering framework for GPT-4

7. **Retrieval-Augmented Generation (RAG)**
   - Theory: Vector databases, semantic search, retrieval pipelines
   - Code: Building RAG system with ChromaDB/Pinecone
   - Project: Document Q&A system for enterprise knowledge base

8. **LLM Deployment & Production**
   - Theory: Model optimization, quantization, serving infrastructure
   - Code: Deploy LLM with FastAPI, vLLM, TensorRT
   - Project: Production chatbot with monitoring & logging

**Module Outcome**: End-to-end LLM expertise from pre-training to production deployment. This module alone is worth ‚Çπ40L+ salaries.

---

### **Module 04: Reinforcement Learning** (5-7 weeks)
**Goal**: Master RL algorithms and apply them to real-world problems (including LLMs)

#### Chapters:
1. **RL Fundamentals**
   - Theory: MDP, rewards, value functions, policies, Bellman equations
   - Code: Implementing MDP solver, value iteration, policy iteration
   - Project: GridWorld navigation agent

2. **Q-Learning & Deep Q-Networks (DQN)**
   - Theory: Q-learning, experience replay, target networks
   - Code: DQN from scratch in PyTorch
   - Project: Train agent to play Atari games (Pong, Breakout)

3. **Policy Gradient Methods**
   - Theory: REINFORCE, policy gradients, advantage functions
   - Code: Vanilla policy gradients implementation
   - Project: CartPole balancing agent

4. **Actor-Critic Methods**
   - Theory: A2C, A3C, combining value and policy learning
   - Code: Implementing A2C from scratch
   - Project: Continuous control tasks (MuJoCo environments)

5. **Proximal Policy Optimization (PPO)**
   - Theory: Trust regions, clipped surrogate objective, PPO algorithm
   - Code: PPO implementation from scratch
   - Project: Humanoid robot walking (PyBullet)

6. **Soft Actor-Critic (SAC)**
   - Theory: Maximum entropy RL, off-policy learning
   - Code: SAC implementation
   - Project: Robotic manipulation tasks

7. **RL for LLMs**
   - Theory: RLHF, reward modeling, PPO for language models
   - Code: Training LLM with human feedback
   - Project: Fine-tune model for helpful, harmless outputs

8. **Multi-Agent RL**
   - Theory: Nash equilibria, cooperative vs competitive settings
   - Code: MAPPO, MADDPG implementations
   - Project: Multi-agent soccer game

**Module Outcome**: Deep RL expertise applicable to robotics, gaming, and LLM alignment. High-demand skill for research roles.

---

### **Module 05: Advanced Topics** (4-6 weeks)
**Goal**: Learn cutting-edge techniques used in production AI systems

#### Chapters:
1. **Diffusion Models**
   - Theory: DDPM, DDIM, score-based models, noise schedules
   - Code: Implementing diffusion model from scratch
   - Project: Image generation with Stable Diffusion fine-tuning

2. **Generative Adversarial Networks (GANs)**
   - Theory: GAN training, loss functions, StyleGAN, BigGAN
   - Code: Progressive GAN implementation
   - Project: High-resolution face generation

3. **Multimodal AI**
   - Theory: CLIP, DALL-E, Flamingo, contrastive learning
   - Code: Building multimodal embedding space
   - Project: Image-text retrieval system

4. **Neural Architecture Search (NAS)**
   - Theory: AutoML, DARTS, efficient NAS
   - Code: Implementing simple NAS algorithm
   - Project: Discovering architectures for custom tasks

5. **Model Compression & Quantization**
   - Theory: Pruning, knowledge distillation, quantization techniques
   - Code: Quantize models to INT8, optimize for edge devices
   - Project: Deploy model on Raspberry Pi / mobile

6. **MLOps for Deep Learning**
   - Theory: Experiment tracking, model versioning, CI/CD for ML
   - Code: MLflow, Weights & Biases, DVC, Kubeflow
   - Project: End-to-end ML pipeline with monitoring

**Module Outcome**: Cutting-edge skills in generative AI, multimodal systems, and production ML. Stand out in interviews.

---

### **Module 06: Interview Preparation & Career Launch** (3-4 weeks)
**Goal**: Land your dream job with a killer portfolio and interview skills

#### Chapters:
1. **Portfolio Projects**
   - Build 3-5 production-grade projects showcasing your skills
   - GitHub repos with clean code, documentation, demos
   - Project ideas: LLM chatbot, image generation app, RL game agent

2. **Deep Learning System Design**
   - Theory: Designing scalable ML systems, architecture decisions
   - Practice: Mock system design interviews
   - Topics: Recommendation systems, search ranking, fraud detection

3. **Coding Interviews for ML**
   - Data structures & algorithms (with ML twist)
   - PyTorch/TensorFlow implementation questions
   - Practice: LeetCode + ML-specific coding problems

4. **ML Theory Interviews**
   - Math fundamentals: Linear algebra, calculus, probability
   - Deep dive: Backprop, optimization, loss functions
   - Common interview questions with detailed answers

5. **Behavioral & Culture Fit**
   - STAR method for behavioral questions
   - Discussing your projects effectively
   - Negotiation strategies for ‚Çπ1 CR+ offers

6. **Resume & Application Strategy**
   - Crafting ML engineer resume that gets interviews
   - Targeting companies: Where to apply, referral strategies
   - LinkedIn optimization for recruiters

**Module Outcome**: Job-ready with polished portfolio, interview confidence, and application strategy. Ready to negotiate ‚Çπ1 CR+ packages.

---

## üõ†Ô∏è Technologies & Tools You'll Master

### **Core Libraries**:
- **PyTorch**: Primary deep learning framework (98% of course)
- **NumPy**: For implementing algorithms from scratch
- **TensorFlow/Keras**: Secondary framework (for comparison)

### **LLM Tools**:
- **Hugging Face Transformers**: Model hub, training, inference
- **LangChain**: LLM application development
- **vLLM**: High-performance LLM serving
- **PEFT**: Parameter-efficient fine-tuning (LoRA, QLoRA)

### **RL Libraries**:
- **Gym/Gymnasium**: RL environments
- **Stable Baselines3**: RL algorithm implementations
- **PyBullet**: Physics simulation for robotics

### **MLOps & Production**:
- **MLflow**: Experiment tracking
- **Weights & Biases**: Advanced experiment management
- **Docker & Kubernetes**: Containerization and orchestration
- **FastAPI**: Deploying ML models as APIs
- **TensorRT**: Model optimization for NVIDIA GPUs

### **Data & Infrastructure**:
- **CUDA**: GPU programming basics
- **Ray**: Distributed computing for ML
- **Apache Spark**: Big data processing
- **ChromaDB/Pinecone**: Vector databases for RAG

---

## üìñ Course Methodology

### **For Each Chapter You Get**:

#### 1. **Theory README** (üìÑ README.md)
- **Mathematical foundations**: All equations explained step-by-step
- **Intuitive explanations**: Why this technique works, when to use it
- **Real-world applications**: How top companies use this
- **Common pitfalls**: Mistakes to avoid
- **Research papers**: Links to original papers with summaries

#### 2. **Implementation Code** (üêç Python file)
- **Line-by-line explanations**: Every single line of code is commented
- **From scratch implementations**: Build everything from first principles
- **PyTorch versions**: Production-ready implementations
- **Visualizations**: Plots, diagrams, training curves
- **Debugging tips**: Common errors and how to fix them

#### 3. **Hands-On Project** (üöÄ Project)
- **Real-world problem**: Practical application of concepts
- **Dataset**: Curated datasets with proper train/val/test splits
- **Evaluation metrics**: Proper benchmarking
- **Hyperparameter tuning**: Systematic optimization
- **Results analysis**: Interpret model behavior

---

## üéì Prerequisites

### **Required**:
- ‚úÖ **Python programming**: Comfortable with Python syntax, functions, classes
- ‚úÖ **Basic mathematics**: High school algebra, basic calculus (derivatives)
- ‚úÖ **Programming logic**: If-else, loops, data structures

### **Recommended (but we'll teach these)**:
- üìö **Linear algebra**: Matrices, vectors, dot products
- üìö **Calculus**: Derivatives, chain rule, gradients
- üìö **Probability**: Distributions, expectations
- üìö **Python libraries**: NumPy, Matplotlib basics

**Don't worry if you lack these!** Every module starts from first principles and explains all mathematical concepts.

---

## üíª Setup & Installation

### **Hardware Requirements**:

**Minimum**:
- CPU: 4+ cores (Intel i5/AMD Ryzen 5 or better)
- RAM: 16GB
- Storage: 50GB free space
- GPU: Not required for initial modules (we'll use Colab)

**Recommended** (for training large models):
- CPU: 8+ cores (Intel i7/AMD Ryzen 7)
- RAM: 32GB
- Storage: 200GB SSD
- GPU: NVIDIA RTX 3060 (12GB VRAM) or better

**Cloud Options**:
- Google Colab (Free tier for learning)
- Kaggle Notebooks (Free GPU)
- AWS SageMaker / Google Cloud AI Platform (for production projects)

### **Software Installation**:

```bash
# Python 3.9+ required
# We recommend using Anaconda/Miniconda

# Create virtual environment
conda create -n dl_master python=3.10
conda activate dl_master

# Install core libraries (per module, detailed in each module's requirements.txt)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers datasets accelerate peft
pip install numpy pandas matplotlib seaborn scikit-learn
pip install jupyter notebook ipywidgets

# For specific modules, install additional requirements
# Module 03 (LLMs): pip install -r modules/03-LLMs/requirements.txt
# Module 04 (RL): pip install gymnasium stable-baselines3
# Module 05 (Advanced): pip install diffusers wandb
```

---

## üöÄ How to Use This Course

### **Recommended Learning Path**:

1. **Follow Sequential Order**: Modules build on each other
2. **Don't Skip Theory**: Read README fully before coding
3. **Type, Don't Copy**: Manually type code to build muscle memory
4. **Experiment**: Modify hyperparameters, try different architectures
5. **Build Projects**: Complete every project, add to GitHub
6. **Review Regularly**: Spaced repetition for long-term retention

### **Time Commitment**:
- **Minimum**: 15-20 hours/week ‚Üí 9-10 months
- **Recommended**: 25-30 hours/week ‚Üí 7-8 months
- **Intensive**: 40+ hours/week ‚Üí 5-6 months

### **Weekly Schedule Example**:
- **Monday-Wednesday**: Theory + Implementation (5-7 hours)
- **Thursday-Friday**: Hands-on projects (5-6 hours)
- **Saturday**: Review, debug, experiment (4-5 hours)
- **Sunday**: Study research papers, watch lectures (3-4 hours)

---

## üìä Success Metrics & Milestones

### **After Module 01** (Week 8):
- ‚úÖ Build neural networks from scratch
- ‚úÖ Train CNNs achieving 90%+ accuracy on CIFAR-10
- ‚úÖ Implement RNNs for text generation

### **After Module 02** (Week 14):
- ‚úÖ Implement transformer from scratch
- ‚úÖ Understand attention mechanisms deeply
- ‚úÖ Build GPT-style language model

### **After Module 03** (Week 22):
- ‚úÖ Fine-tune LLMs with LoRA
- ‚úÖ Build RAG system for document Q&A
- ‚úÖ Deploy LLM to production with FastAPI

### **After Module 04** (Week 29):
- ‚úÖ Implement DQN, PPO, SAC from scratch
- ‚úÖ Train RL agents for Atari games
- ‚úÖ Apply RLHF to language models

### **After Module 05** (Week 35):
- ‚úÖ Generate images with diffusion models
- ‚úÖ Build multimodal AI systems
- ‚úÖ Set up MLOps pipelines

### **After Module 06** (Week 39):
- ‚úÖ Portfolio with 5+ production projects
- ‚úÖ Pass mock interviews confidently
- ‚úÖ **Land job offers with ‚Çπ1 CR+ packages**

---

## üèÜ Career Outcomes & Job Roles

### **Roles You'll Be Ready For**:

1. **Machine Learning Engineer** (‚Çπ25-60L)
   - Build and deploy ML models at scale
   - Companies: Google, Amazon, Microsoft, Flipkart

2. **Deep Learning Engineer** (‚Çπ35-80L)
   - Design neural network architectures
   - Companies: NVIDIA, Meta, OpenAI

3. **LLM Engineer** (‚Çπ50L-1.2CR)
   - Fine-tune and deploy large language models
   - Companies: Anthropic, Cohere, AI startups

4. **Research Scientist** (‚Çπ40L-1CR)
   - Publish papers, advance state-of-the-art
   - Companies: DeepMind, Google Brain, FAIR

5. **AI Architect** (‚Çπ60L-1.5CR)
   - Design end-to-end AI systems
   - Companies: Consulting firms, enterprise AI teams

6. **ML Ops Engineer** (‚Çπ30-70L)
   - Production ML infrastructure
   - Companies: Uber, Netflix, Airbnb

---

## üìö Additional Resources

### **Research Papers** (covered in course):
- "Attention Is All You Need" (Transformers)
- "BERT: Pre-training of Deep Bidirectional Transformers"
- "Language Models are Few-Shot Learners" (GPT-3)
- "Deep Residual Learning for Image Recognition" (ResNet)
- "Proximal Policy Optimization Algorithms" (PPO)

### **Books** (optional supplements):
- Deep Learning by Goodfellow, Bengio, Courville
- Dive into Deep Learning (d2l.ai) - Free online
- Reinforcement Learning: An Introduction by Sutton & Barto

### **Communities**:
- r/MachineLearning, r/deeplearning on Reddit
- Hugging Face Discord
- PyTorch Forums
- Papers with Code

---

## ü§ù Support & Community

- **Questions**: Open GitHub issues for each module
- **Discussion**: Join our Discord community (link in each module)
- **Office Hours**: Live Q&A sessions (schedule TBD)
- **Code Review**: Submit projects for feedback

---

## üìÖ Course Updates

This course is **actively maintained** and updated with:
- Latest architectures (e.g., GPT-4, LLaMA-3)
- New research findings
- Industry best practices
- Student feedback improvements

**Last Updated**: October 2025
**Version**: 1.0.0 (Chapters 1-4 Complete)

---

## üìä Course Progress

### ‚úÖ Completed Modules

| Chapter | Status | Content | Lines of Code |
|---------|--------|---------|---------------|
| **Chapter 01: Neural Networks** | ‚úÖ Complete | Theory + NumPy Implementation + MNIST Project | ~2,600 lines |
| **Chapter 02: Backpropagation** | ‚úÖ Complete | Theory + Custom Autograd + Chain Rule Project | ~1,900 lines |
| **Chapter 03: Optimization** | ‚úÖ Complete | SGD/Momentum/Adam + Visualizer Project | ~1,700 lines |
| **Chapter 04: Regularization** | ‚úÖ Complete | L1/L2/Dropout/BatchNorm + Overfit Detector | ~2,100 lines |

**Total**: 15 files, ~28,000 words of theory, ~8,300 lines of code

### üöß In Progress

- Chapter 05: Convolutional Neural Networks (CNNs)
- Chapter 06: Advanced CNNs & Transfer Learning
- Chapter 07: Recurrent Neural Networks (RNNs)
- Chapter 08: Sequence-to-Sequence Models

### üìÖ Coming Soon

- Module 02: Transformers & Attention Mechanisms (6 chapters)
- Module 03: Large Language Models (8 chapters)
- Module 04: Reinforcement Learning (8 chapters)
- Module 05: Advanced Topics (6 chapters)
- Module 06: Interview Preparation (6 chapters)

---

## ‚ö° Let's Begin Your Journey!

You're about to embark on an incredible learning journey. By the end of this course, you'll have:
- ‚úÖ **Deep expertise** in neural networks, transformers, LLMs, RL
- ‚úÖ **Production skills** to deploy models at scale
- ‚úÖ **Portfolio projects** that impress recruiters
- ‚úÖ **Interview confidence** to ace technical rounds
- ‚úÖ **Career trajectory** toward ‚Çπ1 CR+ packages

**Remember**: The goal isn't just to understand AI‚Äîit's to **build AI systems that solve real problems** and **land your dream job**.

---

## ü§ù Contributing

This is an open-source educational project! Contributions are welcome:

- üêõ **Report bugs**: Open an issue
- üí° **Suggest improvements**: Submit a pull request
- üìö **Share resources**: Add to the learning materials
- ‚≠ê **Star the repo**: Help others discover this course

---

## üìú License

This course is released under the **MIT License**. Feel free to use, modify, and share!

---

## üôè Acknowledgments

This course is built with love for the AI community. Special thanks to:
- The deep learning research community
- Open-source contributors
- Students who provided feedback
- Everyone striving to learn AI from scratch

---

## üìß Contact & Support

- **Issues**: Open a GitHub issue
- **Discussions**: Use GitHub Discussions
- **Updates**: Watch this repo for new chapters

**Happy Learning! Let's build something amazing together! üöÄ**

---

## üìÇ Course Structure

```
teaching_by_doing_AI_course/
‚îÇ
‚îú‚îÄ‚îÄ README.md (this file)
‚îÇ
‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îú‚îÄ‚îÄ 01-Deep-Learning-Fundamentals/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md (Module overview)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chapter_01_neural_networks/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md (Theory)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ neural_network_from_scratch.py (Implementation)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ project_mnist_classifier.py (Project)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chapter_02_backpropagation/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ... (8 chapters total)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ 02-Transformers-Attention/
‚îÇ   ‚îú‚îÄ‚îÄ 03-Large-Language-Models/
‚îÇ   ‚îú‚îÄ‚îÄ 04-Reinforcement-Learning/
‚îÇ   ‚îú‚îÄ‚îÄ 05-Advanced-Topics/
‚îÇ   ‚îî‚îÄ‚îÄ 06-Interview-Prep/
‚îÇ
‚îú‚îÄ‚îÄ datasets/ (Links and download scripts)
‚îú‚îÄ‚îÄ resources/ (Cheat sheets, paper summaries)
‚îî‚îÄ‚îÄ solutions/ (Solutions to exercises)
```

---



